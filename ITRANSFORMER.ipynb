{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11396935,"sourceType":"datasetVersion","datasetId":7137767},{"sourceId":11396942,"sourceType":"datasetVersion","datasetId":7137772},{"sourceId":11490130,"sourceType":"datasetVersion","datasetId":7202417}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"############################################################################################################","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.amp import GradScaler, autocast\nfrom tqdm import tqdm\nimport gc\nimport matplotlib.pyplot as plt\nimport os\nimport pickle\nimport psutil\nimport warnings\nimport math\nfrom scipy.signal import butter, filtfilt, find_peaks\nfrom scipy.interpolate import interp1d\nimport pywt\n\n# Suppress transformer warning\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.modules.transformer\")\n\n# VMD Dependency (simplified placeholder)\nprint(\"Using simplified VMD placeholder.\")\ndef vmd_wrapper(signal, alpha, tau, K, DC, init, tol):\n    modes = np.array([signal / K for _ in range(K)])\n    return modes, None, None\n\n# Bandpass Filter\ndef bandpass_filter(data, lowcut, highcut, fs, order=5):\n    nyquist = fs / 2\n    low = lowcut / nyquist\n    high = highcut / nyquist\n    b, a = butter(order, [low, high], btype='band')\n    filtered = filtfilt(b, a, data, axis=0)\n    return filtered\n\n# MAD (Median Absolute Deviation)\ndef mad(data):\n    return np.median(np.abs(data - np.median(data)))\n\n# Wavelet Denoising with Enhanced Thresholding\ndef wavelet_denoise(data, wavelet='db8', level=6):\n    coeffs = pywt.wavedec(data, wavelet, level=level)\n    sigma = mad(coeffs[-level])\n    uthresh = sigma * np.sqrt(2 * np.log(len(data)))\n    for i in range(1, len(coeffs)):\n        coeffs[i] = pywt.threshold(coeffs[i], value=uthresh, mode='soft')\n    return pywt.waverec(coeffs, wavelet)\n\n# False Nearest Neighbors\ndef false_nearest_neighbors(signal, max_dim=10, tau=1, R_T=15.0):\n    def compute_distance(x, y):\n        return np.sqrt(np.sum((x - y)**2))\n    fnn_ratios = []\n    signal = np.array(signal)\n    for m in range(1, max_dim + 1):\n        phase_space = []\n        for i in range(0, len(signal) - (m - 1) * tau, tau):\n            vec = signal[i:i + m * tau:tau]\n            if len(vec) == m:\n                phase_space.append(vec)\n        if not phase_space:\n            fnn_ratios.append(0)\n            continue\n        phase_space = np.array(phase_space)\n        fnn_count = 0\n        total_count = 0\n        for i in range(len(phase_space)):\n            distances = [compute_distance(phase_space[i], phase_space[j]) for j in range(len(phase_space)) if i != j]\n            if distances:\n                min_dist_idx = np.argmin(distances)\n                nn_idx = min_dist_idx if min_dist_idx < i else min_dist_idx + 1\n                if i + m * tau < len(signal) and nn_idx + m * tau < len(signal):\n                    dist_m = compute_distance(phase_space[i], phase_space[nn_idx])\n                    dist_m1 = abs(signal[i + m * tau] - signal[nn_idx + m * tau])\n                    if dist_m > 0 and dist_m1 / dist_m >= R_T:\n                        fnn_count += 1\n                    total_count += 1\n        fnn_ratio = fnn_count / total_count if total_count > 0 else 0\n        fnn_ratios.append(fnn_ratio)\n        if fnn_ratio < 0.01:\n            break\n    if not fnn_ratios:\n        return [0], 1\n    return fnn_ratios, np.argmin(fnn_ratios) + 1\n\n# Mutual Information\ndef mutual_information(signal, max_tau=50):\n    def entropy(data):\n        counts, _ = np.histogram(data, bins=50, density=True)\n        probs = counts / counts.sum()\n        probs = probs[probs > 0]\n        return -np.sum(probs * np.log2(probs))\n    mi_values = []\n    for tau in range(1, max_tau + 1):\n        s = signal[:-tau]\n        q = signal[tau:]\n        joint = np.vstack((s, q)).T\n        h_s = entropy(s)\n        h_q = entropy(q)\n        h_sq = entropy(joint)\n        mi = h_s + h_q - h_sq\n        mi_values.append(mi)\n        if len(mi_values) > 1 and mi_values[-1] > mi_values[-2]:\n            break\n    return mi_values, np.argmin(mi_values) + 1\n\n# VMD-PSR with Dimensionality Reduction\ndef vmd_psr(data, K=15, embedding_dim=5, tau=1, variate_indices=None, alpha=2000):\n    num_samples, num_variates = data.shape\n    enhanced_data = []\n    for v in range(min(100, num_variates)):  # Limit to 100 variates\n        signal = data[:, v]\n        if variate_indices and v not in variate_indices:\n            cache_file = f\"vmd_psr_variate_{v}.pkl\"\n            if os.path.exists(cache_file):\n                with open(cache_file, 'rb') as f:\n                    modes = pickle.load(f)\n            else:\n                modes = np.array([signal / K for _ in range(K)])\n        else:\n            if len(signal) < 10 or np.all(signal == signal[0]):\n                print(f\"Warning: Skipping variate {v}\")\n                modes = np.array([signal / K for _ in range(K)])\n            else:\n                try:\n                    modes, _, _ = vmd_wrapper(signal, alpha=alpha, tau=0, K=K, DC=0, init=1, tol=1e-6)\n                    if modes.shape != (K, num_samples):\n                        print(f\"Warning: VMD shape mismatch for variate {v}\")\n                        modes_padded = np.zeros((K, num_samples))\n                        modes_padded[:min(K, modes.shape[0]), :min(num_samples, modes.shape[1])] = modes\n                        modes = modes_padded\n                    if variate_indices:\n                        with open(f\"vmd_psr_variate_{v}.pkl\", 'wb') as f:\n                            pickle.dump(modes, f)\n                except Exception as e:\n                    print(f\"Error in VMD for variate {v}: {e}\")\n                    modes = np.array([signal / K for _ in range(K)])\n        for mode in modes:\n            mode_psr = []\n            for d in range(embedding_dim):\n                delay = d * tau\n                delayed = np.roll(mode, -delay)\n                if delay > 0:\n                    delayed[-delay:] = delayed[-delay-1]\n                mode_psr.append(delayed)\n            mode_psr = np.stack(mode_psr, axis=1)\n            enhanced_data.append(mode_psr)\n    return np.concatenate(enhanced_data, axis=1)\n\n# Enhanced iTransformer Model\nclass EnhancediTransformer(nn.Module):\n    def __init__(self, num_variates, seq_len, pred_len, hidden_size=512, num_heads=8, num_layers=6, dropout=0.2):\n        super().__init__()\n        self.pred_len = pred_len\n        self.proj_in = nn.Linear(num_variates, hidden_size)\n        self.pos_enc = PositionalEncoding(hidden_size)\n        self.encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(\n                d_model=hidden_size,\n                nhead=num_heads,\n                dim_feedforward=4*hidden_size,\n                dropout=dropout,\n                activation='gelu',\n                batch_first=True\n            ),\n            num_layers=num_layers\n        )\n        self.proj_out = nn.Sequential(\n            nn.Linear(hidden_size, 256),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(256, pred_len * num_variates)  # Output for all variates\n        )\n\n    def forward(self, x):\n        x = self.proj_in(x)  # [batch, seq_len, hidden]\n        x = self.pos_enc(x)\n        x = self.encoder(x)\n        x = self.proj_out(x[:, -1, :])  # Use last token\n        x = x.view(-1, self.pred_len, x.shape[-1] // self.pred_len)  # Reshape to [batch, pred_len, num_variates]\n        return x  # [batch, pred_len, num_variates]\n\n# Positional Encoding\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        position = torch.arange(max_len).unsqueeze(1).float()\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(1, max_len, d_model)\n        pe[..., 0::2] = torch.sin(position * div_term)\n        pe[..., 1::2] = torch.cos(position * div_term)\n        self.pe = nn.Parameter(pe, requires_grad=False)\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1), :]\n\n# Enhanced Loss Function with Confidence\nclass ECG_loss(nn.Module):\n    def __init__(self, l2_weight=1e-4):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        self.mae = nn.L1Loss()\n        self.l2_weight = l2_weight\n\n    def forward(self, pred, target, model_parameters=None):\n        # Ensure pred and target have compatible shapes\n        if pred.shape != target.shape:\n            print(f\"Warning: Shape mismatch - pred: {pred.shape}, target: {target.shape}\")\n            pred = pred.expand_as(target[:, :, :pred.shape[-1]])  # Match to target variates\n        weights = torch.ones_like(target)\n        gradients = torch.abs(target[:, 1:] - target[:, :-1])\n        weights[:, :-1] += 5 * gradients  # Weight high-gradient areas\n        mse_loss = (weights * (pred - target)**2).mean()\n        mae_loss = (weights * torch.abs(pred - target)).mean()\n        l2_loss = sum(p.pow(2).sum() for p in model_parameters) * self.l2_weight if model_parameters else 0\n        return 0.7 * mse_loss + 0.3 * mae_loss + l2_loss\n\n# Robust Scaler with Clipping\nclass RobustScaler:\n    def __init__(self, data):\n        self.median = np.median(data, axis=0)\n        self.iqr = np.percentile(data, 75, axis=0) - np.percentile(data, 25, axis=0)\n        self.iqr[self.iqr == 0] = 1.0\n\n    def normalize(self, data):\n        return np.clip((data - self.median) / self.iqr, -5, 5)\n\n    def denormalize(self, data):\n        return data * self.iqr + self.median\n\n# TimeSeriesDataset with Additional Augmentation\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, data, seq_len, pred_len):\n        self.features = data.values.astype(np.float32)\n        self.seq_len = seq_len\n        self.pred_len = pred_len\n        self.num_samples = len(data) - seq_len - pred_len + 1\n        if self.num_samples <= 0:\n            raise ValueError(f\"Insufficient data: num_samples = {self.num_samples}, seq_len = {seq_len}, pred_len = {pred_len}\")\n        rng = np.random.default_rng()\n        noise1 = rng.normal(0, 0.01, (self.num_samples, seq_len, self.features.shape[1])).astype(np.float32)\n        scale = rng.uniform(0.95, 1.05, (self.num_samples, 1, 1)).astype(np.float32)\n        noise2 = rng.normal(0, 0.005, (self.num_samples, seq_len, self.features.shape[1])).astype(np.float32)\n        warp_factor = rng.uniform(0.98, 1.02, self.num_samples).astype(np.float32)\n        self.augmented = []\n        time_orig = np.linspace(0, 1, seq_len)\n        for idx in range(self.num_samples):\n            x = self.features[idx:idx + seq_len]\n            time_warped = np.linspace(0, 1, seq_len) * warp_factor[idx]\n            time_warped = time_warped / time_warped[-1] * time_orig[-1]\n            x_warped = np.zeros_like(x)\n            for v in range(x.shape[1]):\n                interp = interp1d(time_orig, x[:, v], kind='linear', fill_value=\"extrapolate\")\n                x_warped[:, v] = interp(time_warped)\n            t = np.linspace(0, 1, seq_len)\n            wander = 0.02 * np.sin(2 * np.pi * 0.5 * t)\n            x = x_warped + wander[:, None] + noise1[idx]\n            x = x * scale[idx] + noise2[idx]\n            x = np.clip(x, -5, 5)\n            self.augmented.append(x)\n\n    def __len__(self):\n        return max(0, self.num_samples)\n    \n    def __getitem__(self, idx):\n        idx = idx % self.num_samples\n        x = self.augmented[idx]\n        y = self.features[idx + self.seq_len:idx + self.seq_len + self.pred_len]\n        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n\n# Metrics with Confidence Interval\ndef calculate_metrics(true, pred, confidence=0.95):\n    rmse = torch.sqrt(torch.mean((true - pred)**2, dim=(0, 1)))\n    mae = torch.mean(torch.abs(true - pred), dim=(0, 1))\n    std_error = torch.std(true - pred, dim=(0, 1))\n    z_score = 1.96  # For 95% confidence\n    conf_interval = z_score * std_error / np.sqrt(true.size(0))\n    return {'RMSE': rmse.tolist(), 'MAE': mae.tolist(), 'Conf_Interval': conf_interval.tolist()}\n\n# Plot Function\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef plot_actual_vs_predicted(inputs, trues, preds, variates_to_plot=1, filename=\"ecg_prediction_qitransformer.png\", fs=250):\n    # Create figure with IEEE-compliant settings\n    plt.figure(figsize=(8, 4), dpi=600)\n    plt.rcParams['font.family'] = 'Times New Roman'\n    plt.rcParams['font.size'] = 10\n    plt.rcParams['axes.linewidth'] = 1.0\n    plt.rcParams['lines.linewidth'] = 1.5\n    \n    # Select data segment to plot (focusing on a representative QRS complex)\n    input_len = inputs.shape[1]\n    pred_len = preds.shape[1]\n    total_len = input_len + pred_len\n    time_steps_full = np.arange(total_len) * (1000 / fs)  # Convert to milliseconds\n    \n    # Find the most dynamic segment (highest variance) to showcase\n    combined = np.concatenate([inputs[0, :, variates_to_plot], preds[0, :, variates_to_plot]])\n    variances = np.array([np.var(combined[i:i+100]) for i in range(len(combined)-100)])\n    start_idx = np.argmax(variances)\n    window_size = 200  # 800 ms window (200 samples at 250Hz)\n    \n    # Extract the segments\n    actual_input = inputs[0, start_idx:start_idx + (window_size - pred_len), variates_to_plot]\n    actual_trues = trues[0, :min(pred_len, window_size - len(actual_input)), variates_to_plot]\n    actual = np.concatenate([actual_input, actual_trues])[:window_size]\n    \n    pred_input = inputs[0, start_idx:start_idx + (window_size - pred_len), variates_to_plot]\n    pred_preds = preds[0, :min(pred_len, window_size - len(pred_input)), variates_to_plot]\n    prediction = np.concatenate([pred_input, pred_preds])[:window_size]\n    \n    time_steps = time_steps_full[start_idx:start_idx + window_size]\n    \n    # Normalize for better visualization\n    actual = (actual - np.min(actual)) / (np.max(actual) - np.min(actual)) * 1.2  # Scale to 0-1.2 mV\n    prediction = (prediction - np.min(prediction)) / (np.max(prediction) - np.min(prediction)) * 1.2\n    \n    # Plot with enhanced styling\n    plt.plot(time_steps, actual, label=\"Ground Truth\", color=\"#2c7bb6\", linewidth=1.8, alpha=0.9)\n    plt.plot(time_steps, prediction, label=\"iTransformer Prediction\", color=\"#d7191c\", linewidth=1.8, linestyle='--', alpha=0.9)\n    \n    # Detect and annotate key features\n    peaks, _ = find_peaks(actual, height=0.6, distance=20)\n    if len(peaks) > 0:\n        qrs_pos = peaks[0]\n        plt.axvline(x=time_steps[qrs_pos], color='gray', linestyle=':', linewidth=1, alpha=0.7)\n        plt.text(time_steps[qrs_pos], 1.25, 'QRS', ha='center', va='bottom', \n                fontsize=9, bbox=dict(facecolor='white', alpha=0.8, edgecolor='none', pad=1))\n        \n        # Highlight QRS complex\n        qrs_start = max(0, qrs_pos - 10)\n        qrs_end = min(window_size-1, qrs_pos + 10)\n        plt.axvspan(time_steps[qrs_start], time_steps[qrs_end], \n                   color='#fdae61', alpha=0.2, label='QRS Complex')\n    \n    # Add shaded region for prediction area\n    pred_start = input_len - start_idx\n    if pred_start > 0 and pred_start < window_size:\n        plt.axvspan(time_steps[pred_start], time_steps[-1], \n                   color='#abd9e9', alpha=0.15, label='Prediction Horizon')\n    \n    # Formatting\n    plt.title(\"ECG Prediction: iTransformer vs Ground Truth\", fontsize=11, pad=12)\n    plt.xlabel(\"Time (ms)\", fontsize=10, labelpad=5)\n    plt.ylabel(\"Normalized Amplitude (mV)\", fontsize=10, labelpad=5)\n    plt.ylim(-0.1, 1.3)\n    plt.xlim(time_steps[0], time_steps[-1])\n    \n    # IEEE-style legend\n    legend = plt.legend(loc='upper right', fontsize=9, frameon=True, \n                       framealpha=1, edgecolor='black', facecolor='white')\n    legend.get_frame().set_linewidth(0.8)\n    \n    # IEEE-style grid and ticks\n    plt.grid(True, linestyle=':', linewidth=0.6, alpha=0.5)\n    plt.tick_params(axis='both', which='both', direction='in', top=True, right=True, width=0.8)\n    \n    # Adjust layout and save\n    plt.tight_layout(pad=1.5)\n    plt.savefig(filename, dpi=600, bbox_inches='tight', format='png')\n    plt.savefig(filename.replace('.png', '.eps'), dpi=600, bbox_inches='tight', format='eps')  # EPS for IEEE\n    plt.close()\n\n\ndef main():\n    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    device_ids = list(range(torch.cuda.device_count()))\n    print(f\"Using device: {device}, GPUs: {device_ids}\")\n\n    # Load and preprocess data\n    raw_data = pd.read_csv(\"/kaggle/input/7309records/7309_arrhythmia_balanced.csv\")\n    raw_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n    raw_data.dropna(inplace=True)\n    print(f\"Raw data shape after dropping NaN: {raw_data.shape}\")\n\n    # Wavelet denoising\n    filtered_data = np.apply_along_axis(wavelet_denoise, 0, raw_data.drop(columns=['type']).values)\n    filtered_data = bandpass_filter(filtered_data, lowcut=0.5, highcut=45.0, fs=360.0)\n    filtered_data = np.clip(filtered_data, np.percentile(filtered_data, 1), np.percentile(filtered_data, 99))\n    filtered_df = pd.DataFrame(filtered_data, columns=raw_data.columns[:-1])\n\n    # Adaptive VMD parameters\n    signal = filtered_df.values[:, 0]\n    entropy = mad(signal)\n    K = max(8, min(15, int(entropy * 10)))\n    alpha = 2000 + entropy * 5000\n\n    # Dynamic embedding dimension\n    _, embedding_dim = false_nearest_neighbors(signal, max_dim=10)\n    _, tau = mutual_information(signal, max_tau=50)\n    print(f\"Selected K={K}, embedding_dim={embedding_dim}, tau={tau}\")\n\n    # VMD-PSR\n    cache_file = f\"vmd_psr_cache_K{K}_dim{embedding_dim}.pkl\"\n    if os.path.exists(cache_file):\n        print(\"Loading cached VMD-PSR...\")\n        with open(cache_file, 'rb') as f:\n            vmd_psr_data = pickle.load(f)\n    else:\n        print(\"Computing VMD-PSR...\")\n        vmd_psr_data = vmd_psr(filtered_df.values, K=K, embedding_dim=embedding_dim, tau=tau, alpha=alpha)\n        with open(cache_file, 'wb') as f:\n            pickle.dump(vmd_psr_data, f)\n\n    modes, _, _ = vmd_wrapper(signal, alpha=alpha, tau=0, K=K, DC=0, init=1, tol=1e-6)\n    # plot_vmd(signal, modes, K)\n    # plot_fnn(signal)\n    # plot_mi(signal)\n\n    vmd_psr_df = pd.DataFrame(vmd_psr_data, columns=[f\"vmd_psr_{i}\" for i in range(vmd_psr_data.shape[1])])\n    data_ma = filtered_df.rolling(window=5).mean().bfill()\n    data_ma.columns = [f\"{col}_ma\" for col in data_ma.columns]\n    enhanced_data = pd.concat([filtered_df, vmd_psr_df, data_ma], axis=1)\n    print(f\"Enhanced data shape: {enhanced_data.shape}, num_variates: {enhanced_data.shape[1]}\")\n\n    # Robust Scaler\n    scaler = RobustScaler(enhanced_data.values[:int(0.7*len(enhanced_data))])\n    norm_data = pd.DataFrame(scaler.normalize(enhanced_data.values), columns=enhanced_data.columns)\n\n    # Config\n    config = {\n        'num_variates': norm_data.shape[1],\n        'seq_len': 250,\n        'pred_len': 50,\n        'hidden_size': 512,\n        'num_heads': 8,\n        'num_layers': 6,\n        'dropout': 0.2,\n        'lr': 5e-5,\n        'batch_size': 64,\n        'epochs': 200,  # Increased epochs\n        'patience': 10,  # Increased patience\n        'grad_clip': 1.0,\n    }\n\n    # Data loaders\n    total_len = len(norm_data)\n    train_end = int(total_len * 0.7)\n    val_end = train_end + int(total_len * 0.15)\n    train_data = norm_data.iloc[:train_end]\n    val_data = norm_data.iloc[train_end:val_end]\n    test_data = norm_data.iloc[val_end:]\n\n    train_dataset = TimeSeriesDataset(train_data, config['seq_len'], config['pred_len'])\n    val_dataset = TimeSeriesDataset(val_data, config['seq_len'], config['pred_len'])\n    test_dataset = TimeSeriesDataset(test_data, config['seq_len'], config['pred_len'])\n    print(f\"Train dataset num_samples: {train_dataset.num_samples}\")\n    print(f\"Val dataset num_samples: {val_dataset.num_samples}\")\n    print(f\"Test dataset num_samples: {test_dataset.num_samples}\")\n\n    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, pin_memory=True, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, pin_memory=True, num_workers=4)\n    print(f\"Train loader length: {len(train_loader)}\")\n    print(f\"Val loader length: {len(val_loader)}\")\n    print(f\"Test loader length: {len(test_loader)}\")\n\n    if len(train_loader) == 0:\n        raise ValueError(\"Train loader is empty. Check dataset size and batch size.\")\n\n    # Model\n    model = EnhancediTransformer(\n        num_variates=config['num_variates'], seq_len=config['seq_len'], pred_len=config['pred_len'],\n        hidden_size=config['hidden_size'], num_heads=config['num_heads'], num_layers=config['num_layers'],\n        dropout=config['dropout']\n    ).to(device)\n\n    if len(device_ids) > 1:\n        model = nn.DataParallel(model, device_ids=device_ids)\n        print(f\"Using DataParallel on GPUs: {device_ids}\")\n\n    optimizer = optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=1e-4)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['epochs'])\n    criterion = ECG_loss(l2_weight=1e-4)\n    grad_scaler = GradScaler('cuda')\n\n    best_val_rmse = float('inf')\n    patience_counter = 0\n    train_losses, val_losses, rmses, maes = [], [], [], []\n    cpu_usage, gpu_usage = [], []\n\n    for epoch in range(config['epochs']):\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        # Train\n        model.train()\n        train_loss = 0\n        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            with autocast('cuda', dtype=torch.float16):\n                pred = model(x)\n                loss = criterion(pred, y)\n            grad_scaler.scale(loss).backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config['grad_clip'])\n            grad_scaler.step(optimizer)\n            grad_scaler.update()\n            train_loss += loss.item()\n        train_loss /= len(train_loader)\n        train_losses.append(train_loss)\n\n        # Validate\n        model.eval()\n        val_loss = 0\n        preds, trues, inputs = [], [], []\n        with torch.no_grad():\n            for x, y in val_loader:\n                x, y = x.to(device), y.to(device)\n                with autocast('cuda', dtype=torch.float16):\n                    pred = model(x)\n                    loss = criterion(pred, y)\n                val_loss += loss.item()\n                preds.append(pred.cpu())\n                trues.append(y.cpu())\n                inputs.append(x.cpu())\n        val_loss /= len(val_loader)\n        val_losses.append(val_loss)\n        preds = torch.cat(preds)\n        trues = torch.cat(trues)\n        inputs = torch.cat(inputs)\n        preds_denorm = scaler.denormalize(preds.numpy())\n        trues_denorm = scaler.denormalize(trues.numpy())\n        inputs_denorm = scaler.denormalize(inputs.numpy())\n        val_metrics = calculate_metrics(torch.tensor(trues_denorm), torch.tensor(preds_denorm))\n        mean_rmse = np.mean(val_metrics['RMSE'])\n        mean_mae = np.mean(val_metrics['MAE'])\n        rmses.append(val_metrics['RMSE'])\n        maes.append(val_metrics['MAE'])\n\n        scheduler.step()\n\n        cpu_usage.append(psutil.cpu_percent())\n        gpu_usage.append(torch.cuda.memory_reserved(0) / torch.cuda.get_device_properties(0).total_memory * 100 if torch.cuda.is_available() else 0)\n\n        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.6f}, Val Loss={val_loss:.6f}, \"\n              f\"Val RMSE={mean_rmse:.6f}, Val MAE={mean_mae:.6f}, Conf Interval={np.mean(val_metrics['Conf_Interval']):.6f}\")\n\n        if mean_rmse < best_val_rmse:\n            best_val_rmse = mean_rmse\n            torch.save(model.state_dict(), \"best_model.pth\")\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= config['patience']:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n\n    # Test\n    model.load_state_dict(torch.load(\"best_model.pth\", weights_only=True))\n    model.eval()\n    test_loss = 0\n    preds, trues, inputs = [], [], []\n    with torch.no_grad():\n        for batch_idx, (x, y) in enumerate(test_loader):\n            x, y = x.to(device), y.to(device)\n            with autocast('cuda', dtype=torch.float16):\n                pred = model(x)\n                loss = criterion(pred, y[:, :, :pred.shape[-1]])  # Match to predicted variates\n            test_loss += loss.item()\n            preds.append(pred.cpu())\n            trues.append(y.cpu())\n            inputs.append(x.cpu())\n            # Enhanced debug\n            pred = pred.to(dtype=torch.float32)  # Convert to float32 for comparison\n            y = y.to(dtype=torch.float32)  # Convert to float32 for comparison\n            print(f\"Batch {batch_idx}: pred shape: {pred.shape}, min: {pred.min().item()}, max: {pred.max().item()}\")\n            print(f\"Batch {batch_idx}: true shape: {y.shape}, min: {y.min().item()}, max: {y.max().item()}\")\n            if torch.allclose(pred, y[:, :, :pred.shape[-1]], rtol=1e-5, atol=1e-5):\n                print(f\"Warning: Batch {batch_idx} - Predictions are identical to ground truth for predicted variates!\")\n    test_loss /= len(test_loader)\n    preds = torch.cat(preds)\n    trues = torch.cat(trues)\n    inputs = torch.cat(inputs)\n    preds_denorm = scaler.denormalize(preds.numpy())\n    trues_denorm = scaler.denormalize(trues.numpy())\n    inputs_denorm = scaler.denormalize(inputs.numpy())\n    test_metrics = calculate_metrics(torch.tensor(trues_denorm), torch.tensor(preds_denorm))\n    mean_test_rmse = np.mean(test_metrics['RMSE'])\n    mean_test_mae = np.mean(test_metrics['MAE'])\n\n    print(\"\\n=== FINAL TEST RESULTS ===\")\n    print(f\"Test Loss: {test_loss:.6f}\")\n    print(f\"Test RMSE: {mean_test_rmse:.6f}\")\n    print(f\"Test MAE: {mean_test_mae:.6f}\")\n    print(f\"Per-variate RMSE: {[f'{x:.6f}' for x in test_metrics['RMSE'][:3]]}\")\n    print(f\"Confidence Interval: {np.mean(test_metrics['Conf_Interval']):.6f}\")\n\n    # Plot actual vs predicted\n    print(\"Generating ECG prediction plot...\")\n    plot_actual_vs_predicted(\n        inputs_denorm, trues_denorm, preds_denorm,\n        variates_to_plot=1,\n        filename=\"ecg_prediction_qitransformer.png\"\n    )\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:46:12.501167Z","iopub.execute_input":"2025-04-20T17:46:12.501725Z","iopub.status.idle":"2025-04-20T18:28:56.369492Z","shell.execute_reply.started":"2025-04-20T17:46:12.501703Z","shell.execute_reply":"2025-04-20T18:28:56.368773Z"}},"outputs":[{"name":"stdout","text":"Using simplified VMD placeholder.\nUsing device: cuda, GPUs: [0, 1]\nRaw data shape after dropping NaN: (7308, 33)\nSelected K=8, embedding_dim=3, tau=1\nComputing VMD-PSR...\nEnhanced data shape: (7308, 832), num_variates: 832\nTrain dataset num_samples: 4816\nVal dataset num_samples: 797\nTest dataset num_samples: 798\nTrain loader length: 76\nVal loader length: 13\nTest loader length: 13\nUsing DataParallel on GPUs: [0, 1]\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 76/76 [00:14<00:00,  5.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=1.022329, Val Loss=4.132458, Val RMSE=0.002041, Val MAE=0.001661, Conf Interval=0.000141\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 76/76 [00:11<00:00,  6.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=1.015279, Val Loss=4.128566, Val RMSE=0.002040, Val MAE=0.001660, Conf Interval=0.000141\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 76/76 [00:11<00:00,  6.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=1.014028, Val Loss=4.120420, Val RMSE=0.002038, Val MAE=0.001657, Conf Interval=0.000141\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 76/76 [00:11<00:00,  6.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=0.999736, Val Loss=4.110842, Val RMSE=0.002036, Val MAE=0.001655, Conf Interval=0.000141\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 76/76 [00:11<00:00,  6.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=0.986428, Val Loss=4.099949, Val RMSE=0.002035, Val MAE=0.001652, Conf Interval=0.000141\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 76/76 [00:12<00:00,  6.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss=0.960037, Val Loss=4.088598, Val RMSE=0.002030, Val MAE=0.001648, Conf Interval=0.000141\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 76/76 [00:12<00:00,  6.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss=0.945774, Val Loss=4.082528, Val RMSE=0.002027, Val MAE=0.001644, Conf Interval=0.000140\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 76/76 [00:12<00:00,  6.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss=0.928208, Val Loss=4.076137, Val RMSE=0.002023, Val MAE=0.001639, Conf Interval=0.000140\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|██████████| 76/76 [00:12<00:00,  6.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss=0.906795, Val Loss=4.069910, Val RMSE=0.002019, Val MAE=0.001635, Conf Interval=0.000140\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|██████████| 76/76 [00:12<00:00,  6.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss=0.893073, Val Loss=4.072704, Val RMSE=0.002018, Val MAE=0.001633, Conf Interval=0.000140\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11: 100%|██████████| 76/76 [00:12<00:00,  6.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss=0.878683, Val Loss=4.068005, Val RMSE=0.002014, Val MAE=0.001629, Conf Interval=0.000139\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12: 100%|██████████| 76/76 [00:12<00:00,  6.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss=0.871945, Val Loss=4.074531, Val RMSE=0.002014, Val MAE=0.001628, Conf Interval=0.000139\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13: 100%|██████████| 76/76 [00:12<00:00,  6.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss=0.858586, Val Loss=4.066952, Val RMSE=0.002011, Val MAE=0.001625, Conf Interval=0.000139\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14: 100%|██████████| 76/76 [00:12<00:00,  6.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss=0.848275, Val Loss=4.075235, Val RMSE=0.002011, Val MAE=0.001625, Conf Interval=0.000139\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15: 100%|██████████| 76/76 [00:12<00:00,  6.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss=0.838856, Val Loss=4.081541, Val RMSE=0.002011, Val MAE=0.001625, Conf Interval=0.000139\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16: 100%|██████████| 76/76 [00:12<00:00,  6.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss=0.827394, Val Loss=4.078063, Val RMSE=0.002008, Val MAE=0.001621, Conf Interval=0.000139\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17: 100%|██████████| 76/76 [00:12<00:00,  6.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss=0.820245, Val Loss=4.092028, Val RMSE=0.002008, Val MAE=0.001621, Conf Interval=0.000139\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18: 100%|██████████| 76/76 [00:12<00:00,  6.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss=0.810855, Val Loss=4.097919, Val RMSE=0.002006, Val MAE=0.001619, Conf Interval=0.000139\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19: 100%|██████████| 76/76 [00:12<00:00,  6.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss=0.806006, Val Loss=4.092917, Val RMSE=0.002004, Val MAE=0.001619, Conf Interval=0.000139\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20: 100%|██████████| 76/76 [00:12<00:00,  6.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss=0.795499, Val Loss=4.110623, Val RMSE=0.002006, Val MAE=0.001619, Conf Interval=0.000139\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21: 100%|██████████| 76/76 [00:12<00:00,  6.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss=0.787302, Val Loss=4.118742, Val RMSE=0.002005, Val MAE=0.001618, Conf Interval=0.000139\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22: 100%|██████████| 76/76 [00:12<00:00,  6.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss=0.779208, Val Loss=4.131795, Val RMSE=0.002006, Val MAE=0.001619, Conf Interval=0.000139\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23: 100%|██████████| 76/76 [00:12<00:00,  6.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss=0.770876, Val Loss=4.132094, Val RMSE=0.002005, Val MAE=0.001618, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24: 100%|██████████| 76/76 [00:12<00:00,  6.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss=0.764655, Val Loss=4.125976, Val RMSE=0.001999, Val MAE=0.001614, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25: 100%|██████████| 76/76 [00:12<00:00,  6.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss=0.757466, Val Loss=4.137862, Val RMSE=0.002001, Val MAE=0.001616, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26: 100%|██████████| 76/76 [00:12<00:00,  6.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss=0.752702, Val Loss=4.145444, Val RMSE=0.002001, Val MAE=0.001616, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27: 100%|██████████| 76/76 [00:12<00:00,  6.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss=0.743798, Val Loss=4.146188, Val RMSE=0.001998, Val MAE=0.001613, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28: 100%|██████████| 76/76 [00:12<00:00,  6.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss=0.740628, Val Loss=4.147155, Val RMSE=0.001998, Val MAE=0.001613, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29: 100%|██████████| 76/76 [00:12<00:00,  6.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss=0.736439, Val Loss=4.150264, Val RMSE=0.001998, Val MAE=0.001614, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30: 100%|██████████| 76/76 [00:12<00:00,  6.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss=0.731166, Val Loss=4.158862, Val RMSE=0.002000, Val MAE=0.001615, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31: 100%|██████████| 76/76 [00:12<00:00,  6.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31: Train Loss=0.728901, Val Loss=4.164444, Val RMSE=0.001999, Val MAE=0.001615, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32: 100%|██████████| 76/76 [00:12<00:00,  6.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32: Train Loss=0.729294, Val Loss=4.160769, Val RMSE=0.001998, Val MAE=0.001614, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33: 100%|██████████| 76/76 [00:12<00:00,  6.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33: Train Loss=0.721445, Val Loss=4.164396, Val RMSE=0.001997, Val MAE=0.001614, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34: 100%|██████████| 76/76 [00:12<00:00,  6.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34: Train Loss=0.717631, Val Loss=4.166675, Val RMSE=0.001997, Val MAE=0.001613, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35: 100%|██████████| 76/76 [00:12<00:00,  6.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35: Train Loss=0.715361, Val Loss=4.175329, Val RMSE=0.002001, Val MAE=0.001617, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36: 100%|██████████| 76/76 [00:12<00:00,  6.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36: Train Loss=0.713759, Val Loss=4.184813, Val RMSE=0.002000, Val MAE=0.001615, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37: 100%|██████████| 76/76 [00:12<00:00,  6.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37: Train Loss=0.706748, Val Loss=4.181702, Val RMSE=0.002000, Val MAE=0.001616, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38: 100%|██████████| 76/76 [00:12<00:00,  6.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38: Train Loss=0.706414, Val Loss=4.181736, Val RMSE=0.001999, Val MAE=0.001615, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39: 100%|██████████| 76/76 [00:12<00:00,  6.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39: Train Loss=0.705494, Val Loss=4.188712, Val RMSE=0.001998, Val MAE=0.001615, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40: 100%|██████████| 76/76 [00:12<00:00,  6.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40: Train Loss=0.703528, Val Loss=4.189374, Val RMSE=0.002000, Val MAE=0.001616, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41: 100%|██████████| 76/76 [00:12<00:00,  5.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41: Train Loss=0.699170, Val Loss=4.187392, Val RMSE=0.001997, Val MAE=0.001613, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42: 100%|██████████| 76/76 [00:12<00:00,  6.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42: Train Loss=0.692817, Val Loss=4.189233, Val RMSE=0.001998, Val MAE=0.001614, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43: 100%|██████████| 76/76 [00:12<00:00,  6.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43: Train Loss=0.694611, Val Loss=4.199841, Val RMSE=0.001999, Val MAE=0.001615, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44: 100%|██████████| 76/76 [00:12<00:00,  6.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44: Train Loss=0.689223, Val Loss=4.191870, Val RMSE=0.001996, Val MAE=0.001613, Conf Interval=0.000137\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45: 100%|██████████| 76/76 [00:12<00:00,  6.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45: Train Loss=0.690378, Val Loss=4.190897, Val RMSE=0.001996, Val MAE=0.001612, Conf Interval=0.000137\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46: 100%|██████████| 76/76 [00:12<00:00,  6.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46: Train Loss=0.682828, Val Loss=4.193616, Val RMSE=0.001994, Val MAE=0.001610, Conf Interval=0.000137\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47: 100%|██████████| 76/76 [00:12<00:00,  6.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47: Train Loss=0.682908, Val Loss=4.192631, Val RMSE=0.001996, Val MAE=0.001613, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48: 100%|██████████| 76/76 [00:12<00:00,  6.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48: Train Loss=0.681672, Val Loss=4.183069, Val RMSE=0.001992, Val MAE=0.001610, Conf Interval=0.000137\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49: 100%|██████████| 76/76 [00:12<00:00,  6.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49: Train Loss=0.682288, Val Loss=4.187618, Val RMSE=0.001992, Val MAE=0.001609, Conf Interval=0.000137\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50: 100%|██████████| 76/76 [00:12<00:00,  6.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50: Train Loss=0.676320, Val Loss=4.191694, Val RMSE=0.001992, Val MAE=0.001610, Conf Interval=0.000137\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51: 100%|██████████| 76/76 [00:12<00:00,  6.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 51: Train Loss=0.674878, Val Loss=4.199371, Val RMSE=0.001997, Val MAE=0.001614, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52: 100%|██████████| 76/76 [00:12<00:00,  6.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 52: Train Loss=0.672129, Val Loss=4.205533, Val RMSE=0.001996, Val MAE=0.001612, Conf Interval=0.000137\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53: 100%|██████████| 76/76 [00:12<00:00,  6.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 53: Train Loss=0.675259, Val Loss=4.193388, Val RMSE=0.001992, Val MAE=0.001609, Conf Interval=0.000137\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54: 100%|██████████| 76/76 [00:12<00:00,  6.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 54: Train Loss=0.670484, Val Loss=4.204041, Val RMSE=0.001995, Val MAE=0.001611, Conf Interval=0.000137\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55: 100%|██████████| 76/76 [00:12<00:00,  6.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 55: Train Loss=0.666358, Val Loss=4.194360, Val RMSE=0.001992, Val MAE=0.001609, Conf Interval=0.000137\n","output_type":"stream"},{"name":"stderr","text":"Epoch 56: 100%|██████████| 76/76 [00:12<00:00,  6.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 56: Train Loss=0.665566, Val Loss=4.206556, Val RMSE=0.001995, Val MAE=0.001612, Conf Interval=0.000137\n","output_type":"stream"},{"name":"stderr","text":"Epoch 57: 100%|██████████| 76/76 [00:12<00:00,  6.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 57: Train Loss=0.667353, Val Loss=4.204909, Val RMSE=0.001997, Val MAE=0.001614, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 58: 100%|██████████| 76/76 [00:12<00:00,  5.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 58: Train Loss=0.664945, Val Loss=4.200916, Val RMSE=0.001992, Val MAE=0.001609, Conf Interval=0.000137\n","output_type":"stream"},{"name":"stderr","text":"Epoch 59: 100%|██████████| 76/76 [00:12<00:00,  6.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 59: Train Loss=0.666919, Val Loss=4.211998, Val RMSE=0.001997, Val MAE=0.001613, Conf Interval=0.000137\n","output_type":"stream"},{"name":"stderr","text":"Epoch 60: 100%|██████████| 76/76 [00:12<00:00,  6.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 60: Train Loss=0.663195, Val Loss=4.204804, Val RMSE=0.001994, Val MAE=0.001611, Conf Interval=0.000137\n","output_type":"stream"},{"name":"stderr","text":"Epoch 61: 100%|██████████| 76/76 [00:12<00:00,  6.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 61: Train Loss=0.661585, Val Loss=4.210944, Val RMSE=0.001997, Val MAE=0.001613, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 62: 100%|██████████| 76/76 [00:12<00:00,  6.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 62: Train Loss=0.661351, Val Loss=4.201164, Val RMSE=0.001994, Val MAE=0.001611, Conf Interval=0.000137\n","output_type":"stream"},{"name":"stderr","text":"Epoch 63: 100%|██████████| 76/76 [00:12<00:00,  6.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 63: Train Loss=0.658641, Val Loss=4.209354, Val RMSE=0.001996, Val MAE=0.001613, Conf Interval=0.000137\n","output_type":"stream"},{"name":"stderr","text":"Epoch 64: 100%|██████████| 76/76 [00:12<00:00,  6.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 64: Train Loss=0.658015, Val Loss=4.217575, Val RMSE=0.001998, Val MAE=0.001614, Conf Interval=0.000138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 65: 100%|██████████| 76/76 [00:12<00:00,  6.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 65: Train Loss=0.656973, Val Loss=4.203051, Val RMSE=0.001995, Val MAE=0.001612, Conf Interval=0.000137\nEarly stopping at epoch 65\nBatch 0: pred shape: torch.Size([64, 50, 832]), min: -1.94140625, max: 2.498046875\nBatch 0: true shape: torch.Size([64, 50, 832]), min: -4.352345943450928, max: 5.0\nBatch 1: pred shape: torch.Size([64, 50, 832]), min: -2.3671875, max: 1.396484375\nBatch 1: true shape: torch.Size([64, 50, 832]), min: -4.326645374298096, max: 5.0\nBatch 2: pred shape: torch.Size([64, 50, 832]), min: -2.607421875, max: 1.86328125\nBatch 2: true shape: torch.Size([64, 50, 832]), min: -5.0, max: 5.0\nBatch 3: pred shape: torch.Size([64, 50, 832]), min: -2.037109375, max: 1.2978515625\nBatch 3: true shape: torch.Size([64, 50, 832]), min: -5.0, max: 5.0\nBatch 4: pred shape: torch.Size([64, 50, 832]), min: -1.9013671875, max: 1.4091796875\nBatch 4: true shape: torch.Size([64, 50, 832]), min: -5.0, max: 5.0\nBatch 5: pred shape: torch.Size([64, 50, 832]), min: -1.4658203125, max: 1.1025390625\nBatch 5: true shape: torch.Size([64, 50, 832]), min: -5.0, max: 5.0\nBatch 6: pred shape: torch.Size([64, 50, 832]), min: -2.51171875, max: 2.185546875\nBatch 6: true shape: torch.Size([64, 50, 832]), min: -5.0, max: 3.927187204360962\nBatch 7: pred shape: torch.Size([64, 50, 832]), min: -2.580078125, max: 2.26171875\nBatch 7: true shape: torch.Size([64, 50, 832]), min: -5.0, max: 5.0\nBatch 8: pred shape: torch.Size([64, 50, 832]), min: -1.892578125, max: 2.744140625\nBatch 8: true shape: torch.Size([64, 50, 832]), min: -5.0, max: 5.0\nBatch 9: pred shape: torch.Size([64, 50, 832]), min: -2.02734375, max: 3.244140625\nBatch 9: true shape: torch.Size([64, 50, 832]), min: -4.352345943450928, max: 5.0\nBatch 10: pred shape: torch.Size([64, 50, 832]), min: -1.5234375, max: 1.984375\nBatch 10: true shape: torch.Size([64, 50, 832]), min: -4.436948776245117, max: 5.0\nBatch 11: pred shape: torch.Size([64, 50, 832]), min: -2.37890625, max: 1.701171875\nBatch 11: true shape: torch.Size([64, 50, 832]), min: -5.0, max: 5.0\nBatch 12: pred shape: torch.Size([30, 50, 832]), min: -2.484375, max: 1.5400390625\nBatch 12: true shape: torch.Size([30, 50, 832]), min: -5.0, max: 5.0\n\n=== FINAL TEST RESULTS ===\nTest Loss: 4.683531\nTest RMSE: 0.002252\nTest MAE: 0.001816\nPer-variate RMSE: ['0.006397', '0.009645', '0.008915']\nConfidence Interval: 0.000156\nGenerating ECG prediction plot...\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}